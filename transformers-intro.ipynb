{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI8SDNZhs28O"
      },
      "source": [
        "# ðŸ¤– Transformer Hands-on (30 minutes)\n",
        "\n",
        "Welcome! In this short interactive session, weâ€™ll explore what a Transformer model does and see it in action as it **completes sentences**.\n",
        "\n",
        "ðŸŽ¯ **Goal**: Get an intuitive idea of what a Transformer is and try using a pre-trained model with just a few lines of code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSE2j8pxs28P"
      },
      "source": [
        "# Install Hugging Face datasets library if needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1) Define the sentence here (modify as desired)\n",
        "sentence = \"Sarah carried a light bag walking under the light\"\n",
        "tokens = sentence.lower().split()\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# 2) Set a small embedding dimension for clarity\n",
        "embed_dim = 8\n",
        "\n",
        "# 3) Simulate original embeddings: one unique vector per word\n",
        "np.random.seed(0)\n",
        "unique_tokens = list(dict.fromkeys(tokens))  # preserve order, remove duplicates\n",
        "token_embeddings = {token: np.random.randn(embed_dim) for token in unique_tokens}\n",
        "\n",
        "# 4) Build embeds array from token_embeddings dict\n",
        "embeds = np.vstack([token_embeddings[token] for token in tokens])\n",
        "\n",
        "# 5) Positional encoding function\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model)//2)) / np.float32(d_model))\n",
        "    angle_rads = np.arange(position)[:, None] * angle_rates[None, :]\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    return angle_rads\n",
        "\n",
        "# 6) Compute positional encodings\n",
        "pos_enc = positional_encoding(len(tokens), embed_dim)\n",
        "\n",
        "# 7) Compute final embeddings = original + positional\n",
        "embeds_final = embeds + pos_enc\n",
        "\n",
        "# 8) Display original, positional, and final embeddings for each token\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"\\nToken '{token}' (position {i}):\")\n",
        "    print(\"  Original embedding   :\", np.round(embeds[i], 3).tolist())\n",
        "    print(\"  Positional encoding  :\", np.round(pos_enc[i], 3).tolist())\n",
        "    print(\"  Final embedding (sum):\", np.round(embeds_final[i], 3).tolist())"
      ],
      "metadata": {
        "id": "uznmVo4VRCSj",
        "outputId": "6fb73908-000a-42bf-dfd2-23a61873feaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['sarah', 'carried', 'a', 'light', 'bag', 'walking', 'under', 'the', 'light']\n",
            "\n",
            "Token 'sarah' (position 0):\n",
            "  Original embedding   : [1.764, 0.4, 0.979, 2.241, 1.868, -0.977, 0.95, -0.151]\n",
            "  Positional encoding  : [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
            "  Final embedding (sum): [1.764, 1.4, 0.979, 3.241, 1.868, 0.023, 0.95, 0.849]\n",
            "\n",
            "Token 'carried' (position 1):\n",
            "  Original embedding   : [-0.103, 0.411, 0.144, 1.454, 0.761, 0.122, 0.444, 0.334]\n",
            "  Positional encoding  : [0.841, 0.54, 0.1, 0.995, 0.01, 1.0, 0.001, 1.0]\n",
            "  Final embedding (sum): [0.738, 0.951, 0.244, 2.449, 0.771, 1.122, 0.445, 1.334]\n",
            "\n",
            "Token 'a' (position 2):\n",
            "  Original embedding   : [1.494, -0.205, 0.313, -0.854, -2.553, 0.654, 0.864, -0.742]\n",
            "  Positional encoding  : [0.909, -0.416, 0.199, 0.98, 0.02, 1.0, 0.002, 1.0]\n",
            "  Final embedding (sum): [2.403, -0.621, 0.512, 0.126, -2.533, 1.653, 0.866, 0.258]\n",
            "\n",
            "Token 'light' (position 3):\n",
            "  Original embedding   : [2.27, -1.454, 0.046, -0.187, 1.533, 1.469, 0.155, 0.378]\n",
            "  Positional encoding  : [0.141, -0.99, 0.296, 0.955, 0.03, 1.0, 0.003, 1.0]\n",
            "  Final embedding (sum): [2.411, -2.444, 0.341, 0.768, 1.563, 2.469, 0.158, 1.378]\n",
            "\n",
            "Token 'bag' (position 4):\n",
            "  Original embedding   : [-0.888, -1.981, -0.348, 0.156, 1.23, 1.202, -0.387, -0.302]\n",
            "  Positional encoding  : [-0.757, -0.654, 0.389, 0.921, 0.04, 0.999, 0.004, 1.0]\n",
            "  Final embedding (sum): [-1.645, -2.634, 0.042, 1.077, 1.27, 2.202, -0.383, 0.698]\n",
            "\n",
            "Token 'walking' (position 5):\n",
            "  Original embedding   : [-1.049, -1.42, -1.706, 1.951, -0.51, -0.438, -1.253, 0.777]\n",
            "  Positional encoding  : [-0.959, 0.284, 0.479, 0.878, 0.05, 0.999, 0.005, 1.0]\n",
            "  Final embedding (sum): [-2.007, -1.136, -1.227, 2.828, -0.46, 0.561, -1.248, 1.777]\n",
            "\n",
            "Token 'under' (position 6):\n",
            "  Original embedding   : [-1.614, -0.213, -0.895, 0.387, -0.511, -1.181, -0.028, 0.428]\n",
            "  Positional encoding  : [-0.279, 0.96, 0.565, 0.825, 0.06, 0.998, 0.006, 1.0]\n",
            "  Final embedding (sum): [-1.893, 0.747, -0.331, 1.212, -0.451, -0.182, -0.022, 1.428]\n",
            "\n",
            "Token 'the' (position 7):\n",
            "  Original embedding   : [0.067, 0.302, -0.634, -0.363, -0.672, -0.36, -0.813, -1.726]\n",
            "  Positional encoding  : [0.657, 0.754, 0.644, 0.765, 0.07, 0.998, 0.007, 1.0]\n",
            "  Final embedding (sum): [0.724, 1.056, 0.01, 0.402, -0.603, 0.638, -0.806, -0.726]\n",
            "\n",
            "Token 'light' (position 8):\n",
            "  Original embedding   : [2.27, -1.454, 0.046, -0.187, 1.533, 1.469, 0.155, 0.378]\n",
            "  Positional encoding  : [0.989, -0.146, 0.717, 0.697, 0.08, 0.997, 0.008, 1.0]\n",
            "  Final embedding (sum): [3.259, -1.6, 0.763, 0.51, 1.613, 2.466, 0.163, 1.378]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "zn_z922XTEM7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['sarah', 'carried', 'a', 'light', 'bag', 'walking', 'under', 'the']\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}"
      ],
      "metadata": {
        "id": "YIXU1LFZmKsw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"sarah carried a light bag walking under the light\"\n",
        "tokens = sentence.lower().split()\n",
        "token_ids = [word2idx[word] for word in tokens]\n",
        "input_tensor = torch.tensor(token_ids)  # shape: (seq_len,)"
      ],
      "metadata": {
        "id": "i0Ba08dSmP5I",
        "outputId": "68081138-45f5-4ec1-a929-3e0a31a68961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-4-30820027.py\", line 4, in <cell line: 0>\n",
            "    input_tensor = torch.tensor(token_ids)  # shape: (seq_len,)\n",
            "/tmp/ipython-input-4-30820027.py:4: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  input_tensor = torch.tensor(token_ids)  # shape: (seq_len,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 8\n",
        "embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_dim)"
      ],
      "metadata": {
        "id": "_vAGbIxtmR4d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded = embedding(input_tensor)  # shape: (seq_len, embed_dim)\n",
        "print(embedded.shape)"
      ],
      "metadata": {
        "id": "-fCplrL7mURW",
        "outputId": "ea375eeb-a6d0-4be8-e3c4-c16b8186446c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in tokens:\n",
        "    idx = word2idx[word]\n",
        "    emb_vector = embedding(torch.tensor(idx))\n",
        "    print(f\"{word} --> {emb_vector.detach()}\")"
      ],
      "metadata": {
        "id": "t3CDMpwomYTC",
        "outputId": "1bf72748-6a21-4062-a475-1dc45c21ab2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sarah --> tensor([-0.0354, -0.0360, -0.0273,  0.4576, -0.5828, -1.9049, -0.4362,  0.7440])\n",
            "carried --> tensor([-0.4080,  0.7927, -0.9235,  0.0528, -1.1435,  1.9432, -0.5535,  0.3547])\n",
            "a --> tensor([-0.2828,  0.3791, -1.0743,  0.5997, -0.2516,  0.7729,  0.2562, -1.2644])\n",
            "light --> tensor([ 1.3198, -0.0903,  1.1957,  0.9336,  0.0647,  0.7454,  3.4284,  0.9872])\n",
            "bag --> tensor([-1.7117,  0.1749, -0.1585,  0.8617, -0.3888,  1.6417,  0.5674, -1.1896])\n",
            "walking --> tensor([ 0.7275,  1.2251,  0.4950,  0.5525, -0.2189,  0.2997, -0.3747,  0.8317])\n",
            "under --> tensor([-0.0642,  0.4200, -0.7438, -0.0858,  0.3095,  0.3209,  0.3215,  0.0050])\n",
            "the --> tensor([-0.8736, -1.7338, -0.7238,  1.9924, -0.1571, -0.0962,  0.4507,  0.2352])\n",
            "light --> tensor([ 1.3198, -0.0903,  1.1957,  0.9336,  0.0647,  0.7454,  3.4284,  0.9872])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "Cqqgl1h2mhwL",
        "outputId": "80977be4-7125-4d43-811e-e825ccf216ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.11/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext) (2.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchtext) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1->torchtext) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# 1. Carica GloVe da torchtext\n",
        "glove = GloVe(name='6B', dim=50)  # usa GloVe 10-dimensional\n"
      ],
      "metadata": {
        "id": "1Xl-3EHNoq_c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Frase di esempio\n",
        "sentence = \"Sarah carried a light bag walking under the light\"\n",
        "tokens = sentence.lower().split()\n",
        "\n",
        "# 3. Estrai e stampa embedding per ogni parola\n",
        "print(\"Word â†’ Embedding vector:\")\n",
        "for word in tokens:\n",
        "    if word in glove.stoi:\n",
        "        emb = glove[word]\n",
        "        print(f\"{word:>10} â†’ {emb[:5]}\")  # first 5 elements for comparing them simply....\n",
        "    else:\n",
        "        print(f\"{word:>10} â†’ [UNK]\")"
      ],
      "metadata": {
        "id": "60CgWtrLzTUT",
        "outputId": "c0ad7fa6-f27d-497c-c1de-954e41cbe750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word â†’ Embedding vector:\n",
            "     sarah â†’ tensor([-0.4671,  1.6665,  0.1289, -0.1402,  0.4243])\n",
            "   carried â†’ tensor([ 0.6284, -0.0887,  0.3397, -0.5304,  0.0704])\n",
            "         a â†’ tensor([ 0.2171,  0.4651, -0.4676,  0.1008,  1.0135])\n",
            "     light â†’ tensor([ 0.0063,  0.4725, -0.0733, -0.0060,  0.3675])\n",
            "       bag â†’ tensor([-0.0282, -0.2216,  0.4478, -0.1850,  0.9992])\n",
            "   walking â†’ tensor([ 0.2787,  0.7067, -0.3070, -0.5401,  0.6306])\n",
            "     under â†’ tensor([ 0.1372, -0.2950, -0.0592, -0.5924,  0.0230])\n",
            "       the â†’ tensor([ 0.4180,  0.2497, -0.4124,  0.1217,  0.3453])\n",
            "     light â†’ tensor([ 0.0063,  0.4725, -0.0733, -0.0060,  0.3675])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # Explicitly import numpy again\n",
        "\n",
        "# Carica GloVe 50-dimensional\n",
        "glove = GloVe(name='6B', dim=50)\n",
        "\n",
        "# Parole da confrontare\n",
        "words = [\"king\", \"queen\", \"man\", \"woman\", \"prince\", \"princess\", \"doctor\", \"nurse\"]\n",
        "\n",
        "# Prendi i vettori dal vocabolario\n",
        "valid_words = [w for w in words if w in glove.stoi]\n",
        "\n",
        "# Check NumPy version to confirm availability\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "vectors = torch.stack([glove[w] for w in valid_words]).numpy()\n",
        "\n",
        "# PCA in 2D\n",
        "pca = PCA(n_components=2)\n",
        "coords = pca.fit_transform(vectors)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, word in enumerate(valid_words):\n",
        "    x, y = coords[i]\n",
        "    plt.scatter(x, y)\n",
        "    plt.text(x + 0.01, y + 0.01, word, fontsize=12)\n",
        "plt.title(\"GloVe Embeddings â€“ relazioni semantiche (PCA)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A51qPbfR2bUQ",
        "outputId": "dcae3e7b-4338-4d05-f780-3e50b08fef39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 2.0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-1486028397.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NumPy version: {np.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# PCA in 2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qv3ZyyYh3yQ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}